## WebCrawler-Pro: Web Scraper mit API & Quellcode | Funktionsreich, Sicher & Frei Nutzbar

ğŸ“Œ **Beschreibung der Software**

ğŸš€ **WebCrawler-Pro: Ihr Web Scraper mit vollem Funktionsumfang â€“ inklusive Quellcode und allen Rechten!**

Sichern Sie sich jetzt WebCrawler-Pro als einmaligen Kauf und optimieren Sie Ihre Datenprozesse dauerhaft! Diese umfassende Web-Scraping-LÃ¶sung, inklusive des kompletten und uneingeschrÃ¤nkten Quellcodes, bietet Organisationen die volle Kontrolle und FlexibilitÃ¤t Ã¼ber die Extraktion, Verarbeitung und Integration von Webdaten. Mit dem Erwerb erhalten Sie alle Rechte zur Nutzung, Anpassung und Weiterentwicklung â€“ ohne versteckte Folgekosten oder LizenzgebÃ¼hren.

WebCrawler-Pro ist ein sofort einsatzbereites System fÃ¼r automatisiertes Web-Data-Mining, bereitgestellt mit einer RESTful API fÃ¼r die nahtlose Integration in Ihre bestehenden Systeme und Workflows. Sie erwerben nicht nur die Software, sondern die volle VerfÃ¼gungsgewalt darÃ¼ber.

ğŸ¯ **Ideal fÃ¼r Organisationen, die maximale Kontrolle, Datensicherheit und Anpassbarkeit suchen und eine sofort nutzbare Basis fÃ¼r ihre individuellen Web-Scraping-Projekte benÃ¶tigen!**

---

## ğŸ”¹ Was WebCrawler-Pro auszeichnet (Kernfunktionen & Vorteile)

*   **Umfassendes Web-Scraping mit aiohttp und Selenium Fallback:**
    *   Nutzt **aiohttp** fÃ¼r hochperformante, asynchrone Web-Requests und **Selenium** als robusten Fallback fÃ¼r dynamische Webseiten.
    *   Extrahiert prÃ¤zise **Textinhalte, relevante Metadaten, Keywords und strukturierte Daten** von modernen Webseiten.
    *   UnterstÃ¼tzt **CSS-Selektoren** fÃ¼r gezielte Datenextraktion und **benutzerdefinierte Processing-Funktionen** fÃ¼r individuelle Datenverarbeitung.

*   **RESTful API fÃ¼r Enterprise-Automatisierung:**
    *   Steuern Sie **sÃ¤mtliche Scraping-Prozesse programmatisch** Ã¼ber die vollumfÃ¤ngliche RESTful API.
    *   Rufen Sie **extrahierte Daten in Echtzeit** ab und integrieren Sie WebCrawler-Pro nahtlos in Ihre bestehenden Systeme und Workflows.
    *   Nutzen Sie die API fÃ¼r die **Automatisierung von Datenerfassung, Datenanalyse und Content-Aggregation** in Enterprise-Umgebungen.

*   **Erweiterte Task-Planung & Datenbank-Verwaltung:**
    *   Definieren und verwalten Sie **wiederkehrende Scraping-Aufgaben (Scheduled Tasks)** mit detaillierter Zeitplanung (stÃ¼ndlich, tÃ¤glich, minÃ¼tlich).
    *   Nutzen Sie das **Streamlit Admin Dashboard** fÃ¼r die einfache Erstellung, Bearbeitung, LÃ¶schung und manuelle AusfÃ¼hrung von Tasks.
    *   **Datenbankbasiertes System mit SQLite:**  Die lokale SQLite-Datenbank ermÃ¶glicht **effiziente Datenspeicherung** von Webseiteninhalten und Task-Konfigurationen.
    *   Integrierter **Datenbankbrowser** Ã¼ber die Streamlit OberflÃ¤che fÃ¼r die direkte Recherche und Analyse der gesammelten Daten.

*   **Leistungsstarkes Caching & Ratenbegrenzung:**
    *   **Integriertes Caching** (konfigurierbar) fÃ¼r optimierte Performance und reduzierte Last auf Zielwebseiten.
    *   **Rate Limiting** (konfigurierbar) zum Schutz der API und zur Einhaltung von Webseiten-Nutzungsbedingungen.

*   **Sicherheitsorientiertes Design & API-Key Authentifizierung:**
    *   **PrimÃ¤r textorientierte Extraktion** mit optionaler HTML/CSS-Datenextraktion minimiert potenzielle Sicherheitsrisiken.
    *   **Gezielter Scraping-Ansatz** fÃ¼r einzelne URLs (keine automatische Unterlink-Verfolgung) fÃ¼r kontrollierte Datenerfassung.
    *   **Umfassende Sicherheitsfunktionen:** API-Key-Authentifizierung, Ratenbegrenzung, CSS-Selektor-Validierung und Pfadvalidierung fÃ¼r Processing-Funktionen.
    *   **Integrierter API-Key Generator (`key_generator.py`)** zur einfachen und sicheren Verwaltung von API-Keys.

*   **Streamlit Admin & Datenbankbrowser Dashboard:**
    *   Intuitive **Web-OberflÃ¤che** fÃ¼r die umfassende Verwaltung von geplanten Tasks und deren StatusÃ¼berwachung.
    *   Integrierter **Datenbankbrowser** fÃ¼r die direkte Analyse und den Export der gescrapten Daten.
    *   **Einfache Bedienung und Task-Konfiguration** Ã¼ber benutzerfreundliche Formulare und OberflÃ¤chenelemente.

*   **VollstÃ¤ndiger Quellcode & UneingeschrÃ¤nkte Kontrolle:**
    *   **Kompletter Quellcode im Lieferumfang** â€“ volle Transparenz und Kontrolle Ã¼ber die Software.
    *   **UneingeschrÃ¤nkte Rechte zur Nutzung, Anpassung & Weiterentwicklung** durch interne Entwickler.
    *   **Kosteneffizientes Modell:** Einmaliger Kauf â€“ keine laufenden LizenzgebÃ¼hren oder versteckten Kosten.
    *   **Maximale EigenstÃ¤ndigkeit & DatensouverÃ¤nitÃ¤t** fÃ¼r Ihre Organisation.

## ğŸ”¹ Warum WebCrawler-Pro wÃ¤hlen?

*   âœ… **Sofort nutzbare Enterprise-Architektur:** Web-Scraping, RESTful API, Datenbankintegration, Task-Planung, Sicherheitsfeatures und Admin-OberflÃ¤che in einem umfassenden System.
*   âœ… **Volle Kontrolle & UnabhÃ¤ngigkeit durch Quellcode-Besitz:** Sichern Sie sich die vollstÃ¤ndige VerfÃ¼gungsgewalt Ã¼ber Ihre Web-Scraping-LÃ¶sung.
*   âœ… **Kosteneffizientes Modell durch einmaligen Kauf:**  Profitieren Sie von langfristiger Kostenersparnis ohne wiederkehrende LizenzgebÃ¼hren oder Abonnements.
*   âœ… **Design mit Fokus auf Sicherheit & Kontrolle:**  Setzen Sie auf eine sichere und transparente LÃ¶sung mit umfassenden Sicherheitsfunktionen und Validierungsmechanismen.
*   âœ… **Detaillierte Dokumentation & Benutzerfreundlichkeit:**  Profitieren Sie von einer umfassenden Dokumentation fÃ¼r maximale EigenstÃ¤ndigkeit bei Wartung, Anpassung und Weiterentwicklung.

## ğŸ’° Verkaufsmodell & RechteÃ¼bertragung

ğŸ“Œ **Preis: Verhandelbar (VHB)** â€“ Kontaktieren Sie uns fÃ¼r ein individuelles Angebot, das auf Ihre spezifischen Anforderungen zugeschnitten ist.

ğŸ“Œ **Einmaliger Kauf â€“ Volle RechteÃ¼bertragung:**

*   **UneingeschrÃ¤nkte Nutzung** innerhalb des Unternehmens oder der Organisation (keine Nutzerlimitierung).
*   **VollstÃ¤ndige Anpassungs- und Modifikationsrechte** â€“ optimieren Sie den Quellcode durch interne Entwicklerteams, um WebCrawler-Pro exakt an Ihre BedÃ¼rfnisse anzupassen.
*   **Kommerzielle Nutzungsrechte & Weiterverkauf:**  Nutzen Sie WebCrawler-Pro intern fÃ¼r kommerzielle Zwecke oder integrieren Sie es in Ihre eigenen kommerziellen Produkte und Dienstleistungen.
*   **Keine LizenzgebÃ¼hren nach dem Kauf:**  Einmalige Investition fÃ¼r dauerhafte Nutzung â€“ ohne versteckte Folgekosten oder laufende GebÃ¼hren.

ğŸ“Œ **Wichtiger Hinweis:** Dies ist ein einmaliger Verkauf des Quellcodes als selbst-gehostete Software-LÃ¶sung. Kein direkter Support oder Wartung nach dem Verkauf inklusive. Eine umfassende Dokumentation und Bedienungsanleitung (diese `README.md`) wird jedoch bereitgestellt, um eine reibungslose interne Wartung, Anpassung und Weiterentwicklung zu ermÃ¶glichen.

## 5. BenutzerfÃ¼hrung, Installation, Funktionsbeschreibung, Fehlerbehebung, FAQ, Glossar, Kontakt & Support

5.  **Datenbank Browser OberflÃ¤che ğŸ–¥ï¸âŒ¨ï¸ â€“ Datenbankinhalte durchsuchen:**

    1.  **Datenbankbrowser starten:** Starten Sie die Streamlit Datenbankbrowser-OberflÃ¤che Ã¼ber die Kommandozeile:

        ```bash
        streamlit run db_browser.py
        ```

        *   Die Datenbankbrowser-OberflÃ¤che ist nun unter der Adresse `http://localhost:8501` erreichbar (kann je nach Streamlit Konfiguration variieren).

    2.  **API-Key eingeben:** Geben Sie auf der Startseite den benÃ¶tigten API-Key ein, um sich zu authentifizieren.

    3.  **Suchparameter festlegen:**
        *   **Suchbegriff:** Geben Sie im Textfeld "Suchbegriff" den Suchbegriff ein, nach dem Sie in der Datenbank suchen mÃ¶chten (z.B. ein Wort, eine URL, ein Teil eines Titels).
        *   **Suchfeld:** WÃ¤hlen Sie im Dropdown-MenÃ¼ "Suchfeld" das Feld aus, in dem gesucht werden soll. VerfÃ¼gbare Optionen sind: `url`, `title`, `meta_description`, `text_content`, `domain`.

    4.  **Suche starten:** Klicken Sie auf den Button "Suchen", um die Datenbankabfrage mit den angegebenen Parametern zu starten.

    5.  **Suchergebnisse anzeigen:**
        *   **DataFrame-Anzeige:** Die Suchergebnisse werden als interaktiver Pandas DataFrame unterhalb des Suchformulars angezeigt. Die Tabelle enthÃ¤lt Spalten fÃ¼r `url`, `title`, `meta_description`, `domain` und (gekÃ¼rzt) `text_content`.
        *   **Keine Ergebnisse:** Wenn keine EintrÃ¤ge gefunden werden, die dem Suchbegriff entsprechen, wird eine entsprechende Meldung "Keine Ergebnisse gefunden." angezeigt.
        *   **Fehlermeldungen:** Bei Fehlern wÃ¤hrend der Datenbankabfrage oder API-Kommunikation werden Fehlermeldungen oberhalb der Suchergebnisse angezeigt, um den Benutzer Ã¼ber Probleme zu informieren.

**Bedienungshinweise fÃ¼r die Admin- und Datenbankbrowser-OberflÃ¤che:**

*   **API-Key erforderlich:**  FÃ¼r den Zugriff auf die Funktionen der Admin- und Datenbankbrowser-OberflÃ¤che ist die Eingabe eines gÃ¼ltigen API-Keys erforderlich. Stellen Sie sicher, dass die API-Keys korrekt in der `.env` Datei oder `config.yaml` konfiguriert sind und der API-Server lÃ¤uft.
*   **Echtzeit-Aktualisierung:**  Ã„nderungen an geplanten Tasks (HinzufÃ¼gen, Aktualisieren, LÃ¶schen) in der Admin-OberflÃ¤che werden in Echtzeit in der Datenbank gespeichert und vom Scheduler berÃ¼cksichtigt.
*   **Browser-Neuladen:**  In einigen FÃ¤llen kann es notwendig sein, die Seite im Browser neu zu laden, um sicherzustellen, dass die aktuellsten Daten und Task-Listen angezeigt werden.
*   **ZeitplÃ¤ne und CSS-Selektoren:**  Achten Sie darauf, ZeitplÃ¤ne im korrekten Format einzugeben und CSS-Selektoren als validen JSON-String zu formatieren, um Validierungsfehler zu vermeiden.
*   **Lange `text_content` Spalten:**  Im Datenbankbrowser wird die Spalte `text_content` aus Performance- und DarstellungsgrÃ¼nden auf die ersten 200 Zeichen gekÃ¼rzt. Um den vollstÃ¤ndigen Textinhalt anzuzeigen, verwenden Sie ein externes Datenbank-Tool oder passen Sie den Code der `db_browser.py` App an.

## 5. Funktionsbeschreibung

### 5.1 Geplante Tasks erstellen (API und Admin-OberflÃ¤che) â•ğŸ“

**Request Body (JSON) fÃ¼r API Task-Erstellung:**

```json
{
  "url": "https://www.example.com",
  "schedule_time": "tÃ¤glich um 08:00",
  "text_only": false,
  "stopwords": "example,test",
  "css_selectors": "{\"title\": \"h1\"}",
  "save_file": true,
  "processing_function_path": "custom_processing.py"
}
```

**Parameter:** `url`, `schedule_time`, `text_only`, `stopwords`, `css_selectors`, `save_file`, `processing_function_path`.

### 5.2 Geplante Tasks aktualisieren (API und Admin-OberflÃ¤che) ğŸ”„ğŸ“

**Request Body (JSON) fÃ¼r API Task-Aktualisierung:**

```json
{
  "schedule_time": "stÃ¼ndlich",
  "stopwords": "neue,stopwÃ¶rter"
}
```

**Parameter:** `task_id` (Pfadparameter), Request Body (JSON) mit zu aktualisierenden Feldern.

### 5.3 Geplante Tasks lÃ¶schen (API und Admin-OberflÃ¤che) âŒğŸ“

**Parameter:** `task_id` (Pfadparameter).

### 5.4 Geplante Tasks manuell ausfÃ¼hren (API und Admin-OberflÃ¤che) â–¶ï¸ğŸ“

**Parameter:** `task_id` (Pfadparameter).

### 5.5 Webseiteninhalt abrufen und extrahieren (API und Kommandozeile) ğŸŒâ¡ï¸ğŸ“„

**Prozessablauf:**

1.  URL-Validierung âœ…
2.  Cache-PrÃ¼fung ğŸ—„ï¸
3.  Webseitenabruf (aiohttp primÃ¤r, Selenium Fallback bei Bedarf) ğŸŒ
4.  HTML-Parsing (Beautiful Soup) ğŸ¥£
5.  Datenextraktion (Text, Titel, Meta-Description, H1-Headings, Keywords, CSS-Daten) ğŸ“„
6.  Benutzerdefinierte Datenverarbeitung (optional) âš™ï¸
7.  Datenbank-Speicherung (SQLite) ğŸ’¾
8.  Datei-Speicherung (optional) ğŸ—‚ï¸
9.  Antwortgenerierung (API) / Ausgabe (Kommandozeile) ğŸ“¤

### 5.6 Keyword-Extraktion ğŸ”‘ğŸ§®

*   Textvorverarbeitung, Stopwortfilterung, alphabetische Filterung, WorthÃ¤ufigkeitszÃ¤hlung, Top-N Keywords.

### 5.7 CSS-Datenextraktion ğŸ§±

*   Einfache und konfigurierte Selektoren (mit `selector`, `type`, `cleanup`).
*   SicherheitsprÃ¼fung fÃ¼r CSS-Selektoren.

### 5.8 Benutzerdefinierte Datenverarbeitung âš™ï¸

*   `process_data(data)` Funktion in Python-Datei definieren.
*   Pfad zur Datei in Programm/Task konfigurieren.
*   Sicherheitswarnung beachten.âš ï¸

## 6. Beispielhafte AnwendungsfÃ¤lle

*   Einmaliges Scrapen Ã¼ber Kommandozeile ğŸš€
*   RegelmÃ¤ÃŸiges Scrapen mit geplantem Task â±ï¸
*   Extrahieren von Produktinformationen mit CSS-Selektoren ğŸ›ï¸
*   Datenanalyse mit benutzerdefinierter Processing-Funktion ğŸ“Š
*   Abrufen von Links Ã¼ber API ğŸ”—
*   Datenbankinhalte mit Streamlit Datenbankbrowser durchsuchen âŒ¨ï¸ğŸ–¥ï¸

## 7. Fehlerbehebung

**HÃ¤ufige Fehlermeldungen und LÃ¶sungen:**

*   "UngÃ¼ltige URL" âŒğŸŒ - ÃœberprÃ¼fen Sie die eingegebene URL auf Korrektheit und Format. Stellen Sie sicher, dass die URL mit `http://` oder `https://` beginnt.
*   "Webseiteninhalt konnte nicht abgerufen werden" âŒ - MÃ¶gliche Ursachen: Webseite nicht erreichbar, Serverprobleme, Netzwerkprobleme, blockiert durch Firewall/Robot.txt. ÃœberprÃ¼fen Sie die Webseite manuell im Browser. ErhÃ¶hen Sie ggf. `max_retries` und `retry_delay` in `config.yaml`.
*   "API-Key fehlt oder ist ungÃ¼ltig." âŒğŸ”‘ - Stellen Sie sicher, dass Sie einen gÃ¼ltigen API-Key im `X-API-Key` Header (API-Anfragen) oder im Streamlit UI eingegeben haben. ÃœberprÃ¼fen Sie die API-Key Konfiguration in `.env` und `config.yaml`. Generieren Sie ggf. neue Keys mit `key_generator.py`.
*   "Rate Limit Ã¼berschritten. Bitte warten Sie eine Minute." â³ - Die API ist ratenlimitiert. Reduzieren Sie die Anfragerate oder erhÃ¶hen Sie `rate_limit_requests_per_minute` in `config.yaml` (mit Vorsicht!).
*   "UngÃ¼ltiges JSON-Format fÃ¼r CSS-Selektoren." âŒğŸ§± - ÃœberprÃ¼fen Sie den JSON-String fÃ¼r CSS-Selektoren auf korrekte Syntax. Verwenden Sie einen JSON-Validator, um Fehler zu finden.
*   "UngÃ¼ltiger Pfad zur Processing-Funktion" âŒâš™ï¸ - Stellen Sie sicher, dass der angegebene Pfad zur Python-Datei der Processing-Funktion korrekt ist und die Datei existiert. Stellen Sie sicher, dass der Pfad relativ zum `processing_functions_dir` in `config.yaml` korrekt ist oder ein absoluter Pfad verwendet wird.
*   "Fehler beim Speichern in die Datenbank" âŒğŸ’¾ - MÃ¶gliche Datenbankfehler. ÃœberprÃ¼fen Sie die Datenbankdatei (`webdata.db`) auf IntegritÃ¤t und Berechtigungen. PrÃ¼fen Sie die Server-Logs auf detailliertere Datenbankfehlermeldungen.
*   "Fehler in der Datenverarbeitungsfunktion" âŒâš™ï¸ - ÃœberprÃ¼fen Sie die Log-Ausgabe auf Fehlermeldungen aus Ihrer benutzerdefinierten Processing-Funktion. Debuggen Sie die Funktion auf Fehler.
*   "Kritischer Datenbankfehler im Scheduled Mode. Programm wird beendet." â˜ ï¸ğŸ’¾ - Ein schwerwiegender Datenbankfehler ist aufgetreten, der den Scheduled Mode beeintrÃ¤chtigt. ÃœberprÃ¼fen Sie die DatenbankintegritÃ¤t und -konfiguration. Starten Sie das Programm neu. PrÃ¼fen Sie die Logs auf detaillierte Fehlermeldungen.

**Log-Level Konfiguration:**

Konfigurierbar in `config.yaml` unter `log_level`.

**VerfÃ¼gbare Log-Level:**

*   `DEBUG` (detaillierteste Protokollierung) ğŸ›
*   `INFO` (Standard) â„¹ï¸
*   `WARNING` âš ï¸
*   `ERROR` âŒ
*   `CRITICAL` â˜ ï¸

**Beispiel `config.yaml` fÃ¼r `DEBUG` Log-Level:**

```yaml
log_level: DEBUG
```

## 8. FAQ (HÃ¤ufig gestellte Fragen)

**F: Wie konfiguriere ich API-Keys?**

A: API-Keys kÃ¶nnen in der `config.yaml` Datei unter `api_keys` als Liste von Strings oder sicherer Ã¼ber Umgebungsvariablen (siehe Abschnitt 3.1) konfiguriert werden. Der empfohlene Weg ist die Verwendung des `key_generator.py` Skripts (siehe Installationsschritt 7).

**F: Wie Ã¤ndere ich das Rate Limit der API?**

A: Das Rate Limit (maximale Anfragen pro Minute) kann in der `config.yaml` Datei unter `rate_limit_requests_per_minute` konfiguriert werden.

**F: Wie lange werden Webseiten im Cache gespeichert?**

A: Die GÃ¼ltigkeitsdauer des Caches (in Sekunden) kann in der `config.yaml` Datei unter `cache_expiry_seconds` konfiguriert werden. StandardmÃ¤ÃŸig sind es 600 Sekunden (10 Minuten).

**F: Wie kann ich geplante Tasks verwalten?**

A: Geplante Tasks kÃ¶nnen Ã¼ber die Streamlit Admin-OberflÃ¤che (empfohlen) oder Ã¼ber die Web-API verwaltet werden (erstellen, aktualisieren, lÃ¶schen, auflisten, manuell ausfÃ¼hren, Status abrufen).

**F: Wo werden die gescrapten Daten gespeichert?**

A: Die gescrapten Daten werden in einer SQLite-Datenbank gespeichert. Der Pfad zur Datenbankdatei kann in der `config.yaml` Datei unter `database_file` konfiguriert werden. StandardmÃ¤ÃŸig ist dies `webdata.db` im Projektverzeichnis.

**F: Kann ich nur Textinhalte extrahieren?**

A: Ja, Sie kÃ¶nnen nur Textinhalte extrahieren, indem Sie die Option `--text` in der Kommandozeile verwenden, `text_only=true` im Request Body fÃ¼r die API-Endpunkte `/fetch-html` und `/fetch-text` setzen oder die Checkbox "Nur Text extrahieren" in der Streamlit Admin-OberflÃ¤che aktivieren.

**F: Wie kann ich benutzerdefinierte StopwÃ¶rter verwenden?**

A: Benutzerdefinierte StopwÃ¶rter kÃ¶nnen als kommagetrennte Liste Ã¼ber die Option `--stopwords` in der Kommandozeile, den Parameter `stopwords` in den API-Endpunkten `/fetch-html` und `/fetch-text` oder das Textfeld "StopwÃ¶rter" in der Streamlit Admin-OberflÃ¤che angegeben werden.

**F: Sind CSS-Selektoren sicher zu verwenden?**

A: WebCrawler-Pro implementiert SicherheitsprÃ¼fungen fÃ¼r CSS-Selektoren, um potenziell unsichere Selektoren zu erkennen und zu verhindern. Dennoch sollten Sie bei der Verwendung von CSS-Selektoren Vorsicht walten lassen und nur vertrauenswÃ¼rdige Selektoren verwenden.

**F: Sind benutzerdefinierte Processing-Funktionen sicher?**

A: Benutzerdefinierte Processing-Funktionen kÃ¶nnen beliebigen Python-Code ausfÃ¼hren. Verwenden Sie diese Funktion nur mit Bedacht und stellen Sie sicher, dass Sie nur vertrauenswÃ¼rdigen Code ausfÃ¼hren, um Sicherheitsrisiken zu vermeiden. WebCrawler-Pro validiert den Pfad zur Processing-Funktion, um unsichere Pfade zu verhindern.

**F: UnterstÃ¼tzt WebCrawler-Pro JavaScript-Rendering?**

A: Ja, WebCrawler-Pro verwendet `aiohttp` fÃ¼r schnelle Abrufe und Selenium und ChromeDriver als Fallback, um auch Webseiten mit dynamischen JavaScript-Inhalten abzurufen und zu verarbeiten.

## 9. Glossar

*   **API (Application Programming Interface):** ğŸŒ Eine Schnittstelle, die es Softwareanwendungen ermÃ¶glicht, miteinander zu kommunizieren. Im Kontext von WebCrawler-Pro ermÃ¶glicht die API den programmatischen Zugriff auf Web-Scraping-FunktionalitÃ¤ten.
*   **CSS-Selektor:** ğŸ§± Ein Muster, das verwendet wird, um HTML-Elemente auf einer Webseite auszuwÃ¤hlen und zu formatieren oder Daten aus diesen Elementen zu extrahieren.
*   **ChromeDriver:** ğŸŒ Ein separates ausfÃ¼hrbares Programm, das von Selenium verwendet wird, um Chrome-Browser zu steuern.
*   **JSON (JavaScript Object Notation):** ğŸ“„ Ein leichtgewichtiges Datenformat, das fÃ¼r den Datenaustausch im Web verwendet wird.
*   **Rate Limiting:** â³ Eine Technik zur Begrenzung der Anzahl von Anfragen, die ein Benutzer oder eine Anwendung innerhalb eines bestimmten Zeitraums an eine API senden kann. Dies dient dem Schutz vor Ãœberlastung und Missbrauch.
*   **Caching:** ğŸ—„ï¸ Eine Technik zur Speicherung hÃ¤ufig abgerufener Daten (z.B. Webseiteninhalte) in einem temporÃ¤ren Speicher (Cache), um den Zugriff zu beschleunigen und die Last auf den ursprÃ¼nglichen Datenquelle zu reduzieren.
*   **Scheduled Task (Geplanter Task):** â±ï¸ğŸ“ Eine Aufgabe, die automatisch zu einem vordefinierten Zeitpunkt oder in regelmÃ¤ÃŸigen Intervallen ausgefÃ¼hrt wird. Im Kontext von WebCrawler-Pro sind geplante Tasks Web-Scraping-Aufgaben, die automatisch nach Zeitplan ausgefÃ¼hrt werden.
*   **Selenium:** ğŸŒ Ein Framework fÃ¼r die Automatisierung von Webbrowsern. WebCrawler-Pro verwendet Selenium als Fallback, um Webseiten dynamisch abzurufen und JavaScript-Inhalte zu rendern, falls der primÃ¤re Abruf mit `aiohttp` fehlschlÃ¤gt.
*   **aiohttp:** ğŸš€ Eine Python-Bibliothek fÃ¼r asynchrone HTTP-Client-/Server-Kommunikation. WebCrawler-Pro verwendet `aiohttp` als primÃ¤re Methode fÃ¼r schnelle und effiziente Webseitenabrufe.


---

ğŸ“© **Interesse geweckt?**

Kontaktieren Sie uns direkt fÃ¼r weitere Informationen, eine persÃ¶nliche Demo oder um Ihr individuelles Angebot anzufordern!

**E-Mail:** ğŸ“§ support@ciphercore.de

**Webseite:** ğŸŒ www.ciphercore.de 

