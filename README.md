## WebCrawler-Pro: Web Scraper mit API & Quellcode | Funktionsreich, Sicher & Frei Nutzbar

üìå **Beschreibung der Software**

üöÄ **WebCrawler-Pro: Ihr Web Scraper mit vollem Funktionsumfang ‚Äì inklusive Quellcode und allen Rechten!**

Sichern Sie sich jetzt WebCrawler-Pro als einmaligen Kauf und optimieren Sie Ihre Datenprozesse dauerhaft! Diese umfassende Web-Scraping-L√∂sung, inklusive des kompletten und uneingeschr√§nkten Quellcodes, bietet Organisationen die volle Kontrolle und Flexibilit√§t √ºber die Extraktion, Verarbeitung und Integration von Webdaten. Mit dem Erwerb erhalten Sie alle Rechte zur Nutzung, Anpassung und Weiterentwicklung ‚Äì ohne versteckte Folgekosten oder Lizenzgeb√ºhren.

WebCrawler-Pro ist ein sofort einsatzbereites System f√ºr automatisiertes Web-Data-Mining, bereitgestellt mit einer RESTful API f√ºr die nahtlose Integration in Ihre bestehenden Systeme und Workflows. Sie erwerben nicht nur die Software, sondern die volle Verf√ºgungsgewalt dar√ºber.

üéØ **Ideal f√ºr Organisationen, die maximale Kontrolle, Datensicherheit und Anpassbarkeit suchen und eine sofort nutzbare Basis f√ºr ihre individuellen Web-Scraping-Projekte ben√∂tigen!**

---

## üîπ Was WebCrawler-Pro auszeichnet (Kernfunktionen & Vorteile)

*   **Umfassendes Web-Scraping mit aiohttp und Selenium Fallback:**
    *   Nutzt **aiohttp** f√ºr hochperformante, asynchrone Web-Requests und **Selenium** als robusten Fallback f√ºr dynamische Webseiten.
    *   Extrahiert pr√§zise **Textinhalte, relevante Metadaten, Keywords und strukturierte Daten** von modernen Webseiten.
    *   Unterst√ºtzt **CSS-Selektoren** f√ºr gezielte Datenextraktion und **benutzerdefinierte Processing-Funktionen** f√ºr individuelle Datenverarbeitung.

*   **RESTful API f√ºr Enterprise-Automatisierung:**
    *   Steuern Sie **s√§mtliche Scraping-Prozesse programmatisch** √ºber die vollumf√§ngliche RESTful API.
    *   Rufen Sie **extrahierte Daten in Echtzeit** ab und integrieren Sie WebCrawler-Pro nahtlos in Ihre bestehenden Systeme und Workflows.
    *   Nutzen Sie die API f√ºr die **Automatisierung von Datenerfassung, Datenanalyse und Content-Aggregation** in Enterprise-Umgebungen.

*   **Erweiterte Task-Planung & Datenbank-Verwaltung:**
    *   Definieren und verwalten Sie **wiederkehrende Scraping-Aufgaben (Scheduled Tasks)** mit detaillierter Zeitplanung (st√ºndlich, t√§glich, min√ºtlich).
    *   Nutzen Sie das **Streamlit Admin Dashboard** f√ºr die einfache Erstellung, Bearbeitung, L√∂schung und manuelle Ausf√ºhrung von Tasks.
    *   **Datenbankbasiertes System mit SQLite:**  Die lokale SQLite-Datenbank erm√∂glicht **effiziente Datenspeicherung** von Webseiteninhalten und Task-Konfigurationen.
    *   Integrierter **Datenbankbrowser** √ºber die Streamlit Oberfl√§che f√ºr die direkte Recherche und Analyse der gesammelten Daten.

*   **Leistungsstarkes Caching & Ratenbegrenzung:**
    *   **Integriertes Caching** (konfigurierbar) f√ºr optimierte Performance und reduzierte Last auf Zielwebseiten.
    *   **Rate Limiting** (konfigurierbar) zum Schutz der API und zur Einhaltung von Webseiten-Nutzungsbedingungen.

*   **Sicherheitsorientiertes Design & API-Key Authentifizierung:**
    *   **Prim√§r textorientierte Extraktion** mit optionaler HTML/CSS-Datenextraktion minimiert potenzielle Sicherheitsrisiken.
    *   **Gezielter Scraping-Ansatz** f√ºr einzelne URLs (keine automatische Unterlink-Verfolgung) f√ºr kontrollierte Datenerfassung.
    *   **Umfassende Sicherheitsfunktionen:** API-Key-Authentifizierung, Ratenbegrenzung, CSS-Selektor-Validierung und Pfadvalidierung f√ºr Processing-Funktionen.
    *   **Integrierter API-Key Generator (`key_generator.py`)** zur einfachen und sicheren Verwaltung von API-Keys.

*   **Streamlit Admin & Datenbankbrowser Dashboard:**
    *   Intuitive **Web-Oberfl√§che** f√ºr die umfassende Verwaltung von geplanten Tasks und deren Status√ºberwachung.
    *   Integrierter **Datenbankbrowser** f√ºr die direkte Analyse und den Export der gescrapten Daten.
    *   **Einfache Bedienung und Task-Konfiguration** √ºber benutzerfreundliche Formulare und Oberfl√§chenelemente.

*   **Vollst√§ndiger Quellcode & Uneingeschr√§nkte Kontrolle:**
    *   **Kompletter Quellcode im Lieferumfang** ‚Äì volle Transparenz und Kontrolle √ºber die Software.
    *   **Uneingeschr√§nkte Rechte zur Nutzung, Anpassung & Weiterentwicklung** durch interne Entwickler.
    *   **Kosteneffizientes Modell:** Einmaliger Kauf ‚Äì keine laufenden Lizenzgeb√ºhren oder versteckten Kosten.
    *   **Maximale Eigenst√§ndigkeit & Datensouver√§nit√§t** f√ºr Ihre Organisation.

## üîπ Warum WebCrawler-Pro w√§hlen?

*   ‚úÖ **Sofort nutzbare Enterprise-Architektur:** Web-Scraping, RESTful API, Datenbankintegration, Task-Planung, Sicherheitsfeatures und Admin-Oberfl√§che in einem umfassenden System.
*   ‚úÖ **Volle Kontrolle & Unabh√§ngigkeit durch Quellcode-Besitz:** Sichern Sie sich die vollst√§ndige Verf√ºgungsgewalt √ºber Ihre Web-Scraping-L√∂sung.
*   ‚úÖ **Kosteneffizientes Modell durch einmaligen Kauf:**  Profitieren Sie von langfristiger Kostenersparnis ohne wiederkehrende Lizenzgeb√ºhren oder Abonnements.
*   ‚úÖ **Design mit Fokus auf Sicherheit & Kontrolle:**  Setzen Sie auf eine sichere und transparente L√∂sung mit umfassenden Sicherheitsfunktionen und Validierungsmechanismen.
*   ‚úÖ **Detaillierte Dokumentation & Benutzerfreundlichkeit:**  Profitieren Sie von einer umfassenden Dokumentation f√ºr maximale Eigenst√§ndigkeit bei Wartung, Anpassung und Weiterentwicklung.

## üí∞ Verkaufsmodell & Rechte√ºbertragung

üìå **Preis: Verhandelbar (VHB)** ‚Äì Kontaktieren Sie uns f√ºr ein individuelles Angebot, das auf Ihre spezifischen Anforderungen zugeschnitten ist.

üìå **Einmaliger Kauf ‚Äì Volle Rechte√ºbertragung:**

*   **Uneingeschr√§nkte Nutzung** innerhalb des Unternehmens oder der Organisation (keine Nutzerlimitierung).
*   **Vollst√§ndige Anpassungs- und Modifikationsrechte** ‚Äì optimieren Sie den Quellcode durch interne Entwicklerteams, um WebCrawler-Pro exakt an Ihre Bed√ºrfnisse anzupassen.
*   **Kommerzielle Nutzungsrechte & Weiterverkauf:**  Nutzen Sie WebCrawler-Pro intern f√ºr kommerzielle Zwecke oder integrieren Sie es in Ihre eigenen kommerziellen Produkte und Dienstleistungen.
*   **Keine Lizenzgeb√ºhren nach dem Kauf:**  Einmalige Investition f√ºr dauerhafte Nutzung ‚Äì ohne versteckte Folgekosten oder laufende Geb√ºhren.

üìå **Wichtiger Hinweis:** Dies ist ein einmaliger Verkauf des Quellcodes als selbst-gehostete Software-L√∂sung. Kein direkter Support oder Wartung nach dem Verkauf inklusive. Eine umfassende Dokumentation und Bedienungsanleitung (diese `README.md`) wird jedoch bereitgestellt, um eine reibungslose interne Wartung, Anpassung und Weiterentwicklung zu erm√∂glichen.

## 5. Benutzerf√ºhrung, Installation, Funktionsbeschreibung, Fehlerbehebung, FAQ, Glossar, Kontakt & Support

5.  **Datenbank Browser Oberfl√§che üñ•Ô∏è‚å®Ô∏è ‚Äì Datenbankinhalte durchsuchen:**

    1.  **Datenbankbrowser starten:** Starten Sie die Streamlit Datenbankbrowser-Oberfl√§che √ºber die Kommandozeile:

        ```bash
        streamlit run db_browser.py
        ```

        *   Die Datenbankbrowser-Oberfl√§che ist nun unter der Adresse `http://localhost:8501` erreichbar (kann je nach Streamlit Konfiguration variieren).

    2.  **API-Key eingeben:** Geben Sie auf der Startseite den ben√∂tigten API-Key ein, um sich zu authentifizieren.

    3.  **Suchparameter festlegen:**
        *   **Suchbegriff:** Geben Sie im Textfeld "Suchbegriff" den Suchbegriff ein, nach dem Sie in der Datenbank suchen m√∂chten (z.B. ein Wort, eine URL, ein Teil eines Titels).
        *   **Suchfeld:** W√§hlen Sie im Dropdown-Men√º "Suchfeld" das Feld aus, in dem gesucht werden soll. Verf√ºgbare Optionen sind: `url`, `title`, `meta_description`, `text_content`, `domain`.

    4.  **Suche starten:** Klicken Sie auf den Button "Suchen", um die Datenbankabfrage mit den angegebenen Parametern zu starten.

    5.  **Suchergebnisse anzeigen:**
        *   **DataFrame-Anzeige:** Die Suchergebnisse werden als interaktiver Pandas DataFrame unterhalb des Suchformulars angezeigt. Die Tabelle enth√§lt Spalten f√ºr `url`, `title`, `meta_description`, `domain` und (gek√ºrzt) `text_content`.
        *   **Keine Ergebnisse:** Wenn keine Eintr√§ge gefunden werden, die dem Suchbegriff entsprechen, wird eine entsprechende Meldung "Keine Ergebnisse gefunden." angezeigt.
        *   **Fehlermeldungen:** Bei Fehlern w√§hrend der Datenbankabfrage oder API-Kommunikation werden Fehlermeldungen oberhalb der Suchergebnisse angezeigt, um den Benutzer √ºber Probleme zu informieren.

**Bedienungshinweise f√ºr die Admin- und Datenbankbrowser-Oberfl√§che:**

*   **API-Key erforderlich:**  F√ºr den Zugriff auf die Funktionen der Admin- und Datenbankbrowser-Oberfl√§che ist die Eingabe eines g√ºltigen API-Keys erforderlich. Stellen Sie sicher, dass die API-Keys korrekt in der `.env` Datei oder `config.yaml` konfiguriert sind und der API-Server l√§uft.
*   **Echtzeit-Aktualisierung:**  √Ñnderungen an geplanten Tasks (Hinzuf√ºgen, Aktualisieren, L√∂schen) in der Admin-Oberfl√§che werden in Echtzeit in der Datenbank gespeichert und vom Scheduler ber√ºcksichtigt.
*   **Browser-Neuladen:**  In einigen F√§llen kann es notwendig sein, die Seite im Browser neu zu laden, um sicherzustellen, dass die aktuellsten Daten und Task-Listen angezeigt werden.
*   **Zeitpl√§ne und CSS-Selektoren:**  Achten Sie darauf, Zeitpl√§ne im korrekten Format einzugeben und CSS-Selektoren als validen JSON-String zu formatieren, um Validierungsfehler zu vermeiden.
*   **Lange `text_content` Spalten:**  Im Datenbankbrowser wird die Spalte `text_content` aus Performance- und Darstellungsgr√ºnden auf die ersten 200 Zeichen gek√ºrzt. Um den vollst√§ndigen Textinhalt anzuzeigen, verwenden Sie ein externes Datenbank-Tool oder passen Sie den Code der `db_browser.py` App an.

## 5. Funktionsbeschreibung

### 5.1 Geplante Tasks erstellen (API und Admin-Oberfl√§che) ‚ûïüìù

**Request Body (JSON) f√ºr API Task-Erstellung:**

```json
{
  "url": "https://www.example.com",
  "schedule_time": "t√§glich um 08:00",
  "text_only": false,
  "stopwords": "example,test",
  "css_selectors": "{\"title\": \"h1\"}",
  "save_file": true,
  "processing_function_path": "custom_processing.py"
}
```

**Parameter:** `url`, `schedule_time`, `text_only`, `stopwords`, `css_selectors`, `save_file`, `processing_function_path`.

### 5.2 Geplante Tasks aktualisieren (API und Admin-Oberfl√§che) üîÑüìù

**Request Body (JSON) f√ºr API Task-Aktualisierung:**

```json
{
  "schedule_time": "st√ºndlich",
  "stopwords": "neue,stopw√∂rter"
}
```

**Parameter:** `task_id` (Pfadparameter), Request Body (JSON) mit zu aktualisierenden Feldern.

### 5.3 Geplante Tasks l√∂schen (API und Admin-Oberfl√§che) ‚ùåüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.4 Geplante Tasks manuell ausf√ºhren (API und Admin-Oberfl√§che) ‚ñ∂Ô∏èüìù

**Parameter:** `task_id` (Pfadparameter).

### 5.5 Webseiteninhalt abrufen und extrahieren (API und Kommandozeile) üåê‚û°Ô∏èüìÑ

**Prozessablauf:**

1.  URL-Validierung ‚úÖ
2.  Cache-Pr√ºfung üóÑÔ∏è
3.  Webseitenabruf (aiohttp prim√§r, Selenium Fallback bei Bedarf) üåê
4.  HTML-Parsing (Beautiful Soup) ü•£
5.  Datenextraktion (Text, Titel, Meta-Description, H1-Headings, Keywords, CSS-Daten) üìÑ
6.  Benutzerdefinierte Datenverarbeitung (optional) ‚öôÔ∏è
7.  Datenbank-Speicherung (SQLite) üíæ
8.  Datei-Speicherung (optional) üóÇÔ∏è
9.  Antwortgenerierung (API) / Ausgabe (Kommandozeile) üì§

### 5.6 Keyword-Extraktion üîëüßÆ

*   Textvorverarbeitung, Stopwortfilterung, alphabetische Filterung, Worth√§ufigkeitsz√§hlung, Top-N Keywords.

### 5.7 CSS-Datenextraktion üß±

*   Einfache und konfigurierte Selektoren (mit `selector`, `type`, `cleanup`).
*   Sicherheitspr√ºfung f√ºr CSS-Selektoren.

### 5.8 Benutzerdefinierte Datenverarbeitung ‚öôÔ∏è

*   `process_data(data)` Funktion in Python-Datei definieren.
*   Pfad zur Datei in Programm/Task konfigurieren.
*   Sicherheitswarnung beachten.‚ö†Ô∏è

## 6. Beispielhafte Anwendungsf√§lle

*   Einmaliges Scrapen √ºber Kommandozeile üöÄ
*   Regelm√§√üiges Scrapen mit geplantem Task ‚è±Ô∏è
*   Extrahieren von Produktinformationen mit CSS-Selektoren üõçÔ∏è
*   Datenanalyse mit benutzerdefinierter Processing-Funktion üìä
*   Abrufen von Links √ºber API üîó
*   Datenbankinhalte mit Streamlit Datenbankbrowser durchsuchen ‚å®Ô∏èüñ•Ô∏è

## 7. Fehlerbehebung

**H√§ufige Fehlermeldungen und L√∂sungen:**

*   "Ung√ºltige URL" ‚ùåüåê - √úberpr√ºfen Sie die eingegebene URL auf Korrektheit und Format. Stellen Sie sicher, dass die URL mit `http://` oder `https://` beginnt.
*   "Webseiteninhalt konnte nicht abgerufen werden" ‚ùå - M√∂gliche Ursachen: Webseite nicht erreichbar, Serverprobleme, Netzwerkprobleme, blockiert durch Firewall/Robot.txt. √úberpr√ºfen Sie die Webseite manuell im Browser. Erh√∂hen Sie ggf. `max_retries` und `retry_delay` in `config.yaml`.
*   "API-Key fehlt oder ist ung√ºltig." ‚ùåüîë - Stellen Sie sicher, dass Sie einen g√ºltigen API-Key im `X-API-Key` Header (API-Anfragen) oder im Streamlit UI eingegeben haben. √úberpr√ºfen Sie die API-Key Konfiguration in `.env` und `config.yaml`. Generieren Sie ggf. neue Keys mit `key_generator.py`.
*   "Rate Limit √ºberschritten. Bitte warten Sie eine Minute." ‚è≥ - Die API ist ratenlimitiert. Reduzieren Sie die Anfragerate oder erh√∂hen Sie `rate_limit_requests_per_minute` in `config.yaml` (mit Vorsicht!).
*   "Ung√ºltiges JSON-Format f√ºr CSS-Selektoren." ‚ùåüß± - √úberpr√ºfen Sie den JSON-String f√ºr CSS-Selektoren auf korrekte Syntax. Verwenden Sie einen JSON-Validator, um Fehler zu finden.
*   "Ung√ºltiger Pfad zur Processing-Funktion" ‚ùå‚öôÔ∏è - Stellen Sie sicher, dass der angegebene Pfad zur Python-Datei der Processing-Funktion korrekt ist und die Datei existiert. Stellen Sie sicher, dass der Pfad relativ zum `processing_functions_dir` in `config.yaml` korrekt ist oder ein absoluter Pfad verwendet wird.
*   "Fehler beim Speichern in die Datenbank" ‚ùåüíæ - M√∂gliche Datenbankfehler. √úberpr√ºfen Sie die Datenbankdatei (`webdata.db`) auf Integrit√§t und Berechtigungen. Pr√ºfen Sie die Server-Logs auf detailliertere Datenbankfehlermeldungen.
*   "Fehler in der Datenverarbeitungsfunktion" ‚ùå‚öôÔ∏è - √úberpr√ºfen Sie die Log-Ausgabe auf Fehlermeldungen aus Ihrer benutzerdefinierten Processing-Funktion. Debuggen Sie die Funktion auf Fehler.
*   "Kritischer Datenbankfehler im Scheduled Mode. Programm wird beendet." ‚ò†Ô∏èüíæ - Ein schwerwiegender Datenbankfehler ist aufgetreten, der den Scheduled Mode beeintr√§chtigt. √úberpr√ºfen Sie die Datenbankintegrit√§t und -konfiguration. Starten Sie das Programm neu. Pr√ºfen Sie die Logs auf detaillierte Fehlermeldungen.

**Log-Level Konfiguration:**

Konfigurierbar in `config.yaml` unter `log_level`.

**Verf√ºgbare Log-Level:**

*   `DEBUG` (detaillierteste Protokollierung) üêõ
*   `INFO` (Standard) ‚ÑπÔ∏è
*   `WARNING` ‚ö†Ô∏è
*   `ERROR` ‚ùå
*   `CRITICAL` ‚ò†Ô∏è

**Beispiel `config.yaml` f√ºr `DEBUG` Log-Level:**

```yaml
log_level: DEBUG
```

## 8. FAQ (H√§ufig gestellte Fragen)

**F: Wie konfiguriere ich API-Keys?**

A: API-Keys k√∂nnen in der `config.yaml` Datei unter `api_keys` als Liste von Strings oder sicherer √ºber Umgebungsvariablen (siehe Abschnitt 3.1) konfiguriert werden. Der empfohlene Weg ist die Verwendung des `key_generator.py` Skripts (siehe Installationsschritt 7).

**F: Wie √§ndere ich das Rate Limit der API?**

A: Das Rate Limit (maximale Anfragen pro Minute) kann in der `config.yaml` Datei unter `rate_limit_requests_per_minute` konfiguriert werden.

**F: Wie lange werden Webseiten im Cache gespeichert?**

A: Die G√ºltigkeitsdauer des Caches (in Sekunden) kann in der `config.yaml` Datei unter `cache_expiry_seconds` konfiguriert werden. Standardm√§√üig sind es 600 Sekunden (10 Minuten).

**F: Wie kann ich geplante Tasks verwalten?**

A: Geplante Tasks k√∂nnen √ºber die Streamlit Admin-Oberfl√§che (empfohlen) oder √ºber die Web-API verwaltet werden (erstellen, aktualisieren, l√∂schen, auflisten, manuell ausf√ºhren, Status abrufen).

**F: Wo werden die gescrapten Daten gespeichert?**

A: Die gescrapten Daten werden in einer SQLite-Datenbank gespeichert. Der Pfad zur Datenbankdatei kann in der `config.yaml` Datei unter `database_file` konfiguriert werden. Standardm√§√üig ist dies `webdata.db` im Projektverzeichnis.

**F: Kann ich nur Textinhalte extrahieren?**

A: Ja, Sie k√∂nnen nur Textinhalte extrahieren, indem Sie die Option `--text` in der Kommandozeile verwenden, `text_only=true` im Request Body f√ºr die API-Endpunkte `/fetch-html` und `/fetch-text` setzen oder die Checkbox "Nur Text extrahieren" in der Streamlit Admin-Oberfl√§che aktivieren.

**F: Wie kann ich benutzerdefinierte Stopw√∂rter verwenden?**

A: Benutzerdefinierte Stopw√∂rter k√∂nnen als kommagetrennte Liste √ºber die Option `--stopwords` in der Kommandozeile, den Parameter `stopwords` in den API-Endpunkten `/fetch-html` und `/fetch-text` oder das Textfeld "Stopw√∂rter" in der Streamlit Admin-Oberfl√§che angegeben werden.

**F: Sind CSS-Selektoren sicher zu verwenden?**

A: WebCrawler-Pro implementiert Sicherheitspr√ºfungen f√ºr CSS-Selektoren, um potenziell unsichere Selektoren zu erkennen und zu verhindern. Dennoch sollten Sie bei der Verwendung von CSS-Selektoren Vorsicht walten lassen und nur vertrauensw√ºrdige Selektoren verwenden.

**F: Sind benutzerdefinierte Processing-Funktionen sicher?**

A: Benutzerdefinierte Processing-Funktionen k√∂nnen beliebigen Python-Code ausf√ºhren. Verwenden Sie diese Funktion nur mit Bedacht und stellen Sie sicher, dass Sie nur vertrauensw√ºrdigen Code ausf√ºhren, um Sicherheitsrisiken zu vermeiden. WebCrawler-Pro validiert den Pfad zur Processing-Funktion, um unsichere Pfade zu verhindern.

**F: Unterst√ºtzt WebCrawler-Pro JavaScript-Rendering?**

A: Ja, WebCrawler-Pro verwendet `aiohttp` f√ºr schnelle Abrufe und Selenium und ChromeDriver als Fallback, um auch Webseiten mit dynamischen JavaScript-Inhalten abzurufen und zu verarbeiten.

## 9. Glossar

*   **API (Application Programming Interface):** üåê Eine Schnittstelle, die es Softwareanwendungen erm√∂glicht, miteinander zu kommunizieren. Im Kontext von WebCrawler-Pro erm√∂glicht die API den programmatischen Zugriff auf Web-Scraping-Funktionalit√§ten.
*   **CSS-Selektor:** üß± Ein Muster, das verwendet wird, um HTML-Elemente auf einer Webseite auszuw√§hlen und zu formatieren oder Daten aus diesen Elementen zu extrahieren.
*   **ChromeDriver:** üåê Ein separates ausf√ºhrbares Programm, das von Selenium verwendet wird, um Chrome-Browser zu steuern.
*   **JSON (JavaScript Object Notation):** üìÑ Ein leichtgewichtiges Datenformat, das f√ºr den Datenaustausch im Web verwendet wird.
*   **Rate Limiting:** ‚è≥ Eine Technik zur Begrenzung der Anzahl von Anfragen, die ein Benutzer oder eine Anwendung innerhalb eines bestimmten Zeitraums an eine API senden kann. Dies dient dem Schutz vor √úberlastung und Missbrauch.
*   **Caching:** üóÑÔ∏è Eine Technik zur Speicherung h√§ufig abgerufener Daten (z.B. Webseiteninhalte) in einem tempor√§ren Speicher (Cache), um den Zugriff zu beschleunigen und die Last auf den urspr√ºnglichen Datenquelle zu reduzieren.
*   **Scheduled Task (Geplanter Task):** ‚è±Ô∏èüìù Eine Aufgabe, die automatisch zu einem vordefinierten Zeitpunkt oder in regelm√§√üigen Intervallen ausgef√ºhrt wird. Im Kontext von WebCrawler-Pro sind geplante Tasks Web-Scraping-Aufgaben, die automatisch nach Zeitplan ausgef√ºhrt werden.
*   **Selenium:** üåê Ein Framework f√ºr die Automatisierung von Webbrowsern. WebCrawler-Pro verwendet Selenium als Fallback, um Webseiten dynamisch abzurufen und JavaScript-Inhalte zu rendern, falls der prim√§re Abruf mit `aiohttp` fehlschl√§gt.
*   **aiohttp:** üöÄ Eine Python-Bibliothek f√ºr asynchrone HTTP-Client-/Server-Kommunikation. WebCrawler-Pro verwendet `aiohttp` als prim√§re Methode f√ºr schnelle und effiziente Webseitenabrufe.


---

üì© **Interesse geweckt?**

Kontaktieren Sie uns direkt f√ºr weitere Informationen, eine pers√∂nliche Demo oder um Ihr individuelles Angebot anzufordern!

**E-Mail:** üìß support@ciphercore.de

**Webseite:** üåê www.ciphercore.de 

