# WebCrawler-Pro: Web Scraper mit API & Quellcode | Funktionsreich, Sicher & Frei Nutzbar

## ğŸ“Œ Beschreibung der Software

ğŸš€ **WebCrawler-Pro: Ihr Web Scraper mit vollem Funktionsumfang â€“ inklusive Quellcode und allen Rechten!**

Sichern Sie sich jetzt WebCrawler-Pro als einmaligen Kauf und optimieren Sie Ihre Datenprozesse dauerhaft! Diese umfassende Web-Scraping-LÃ¶sung, inklusive des kompletten und uneingeschrÃ¤nkten Quellcodes, bietet Organisationen die volle Kontrolle und FlexibilitÃ¤t Ã¼ber die Extraktion, Verarbeitung und Integration von Webdaten. Mit dem Erwerb erhalten Sie alle Rechte zur Nutzung, Anpassung und Weiterentwicklung â€“ ohne versteckte Folgekosten oder LizenzgebÃ¼hren.

WebCrawler-Pro ist ein sofort einsatzbereites System fÃ¼r automatisiertes Web-Data-Mining, bereitgestellt mit einer **RESTful API** fÃ¼r die nahtlose Integration in Ihre bestehenden Systeme und Workflows. Sie erwerben nicht nur die Software, sondern die volle VerfÃ¼gungsgewalt darÃ¼ber.

ğŸ¯ **Ideal fÃ¼r Organisationen, die maximale Kontrolle, Datensicherheit und Anpassbarkeit suchen und eine sofort nutzbare Basis fÃ¼r ihre individuellen Web-Scraping-Projekte benÃ¶tigen!**

## ğŸ”¹ Was WebCrawler-Pro auszeichnet (Kernfunktionen & Vorteile)

- **Web-Scraping mit Selenium:** Automatisieren Sie die prÃ¤zise Extraktion von Textinhalten, relevanten Metadaten und Keywords â€“ auch von dynamischen und interaktiven Webseiten.
- **RESTful API fÃ¼r umfassende Automatisierung:** Steuern Sie sÃ¤mtliche Scraping-Prozesse programmatisch Ã¼ber die API und rufen Sie extrahierte Daten in Echtzeit ab.
- **Task-Planung & Monitoring Ã¼ber Datenbank (SQLite):** Definieren und verwalten Sie wiederkehrende Scraping-Aufgaben mit StatusÃ¼berwachung.
- **Datenbankbasiertes System mit Caching:** Die lokale SQLite-Datenbank ermÃ¶glicht effiziente Datenspeicherung und optimierte Performance durch integriertes Caching.

### ğŸ”¹ Sicherheitsorientiertes Design mit Fokus auf Kontrolle

- **PrimÃ¤r textorientierte Extraktion mit optionalen HTML/CSS-Daten**
- **Gezielter Scraping-Ansatz fÃ¼r einzelne URLs** (keine automatische Unterlink-Verfolgung)
- **Integrierte Sicherheitsfunktionen:** API-Key-Authentifizierung, Ratenbegrenzung und Selektor-Validierung
- **Streamlit Admin Dashboard:** Verwaltung von geplanten Tasks und Analyse von Logs Ã¼ber eine Web-OberflÃ¤che
- **VollstÃ¤ndiger Quellcode im Lieferumfang** â€“ uneingeschrÃ¤nkte Kontrolle fÃ¼r eigene Weiterentwicklung

## ğŸ”¹ Warum WebCrawler-Pro wÃ¤hlen?

âœ… **Sofort nutzbare Architektur:** Web-Scraping, API, Datenbankintegration, Task-Planung und Sicherheitsfeatures in einem System.
âœ… **Volle Kontrolle & UnabhÃ¤ngigkeit durch Quellcode-Besitz**
âœ… **Kosteneffizientes Modell durch einmaligen Kauf** â€“ keine LizenzgebÃ¼hren oder Abonnements.
âœ… **Design mit Fokus auf Sicherheit & Kontrolle**
âœ… **Detaillierte Dokumentation fÃ¼r maximale EigenstÃ¤ndigkeit**

## ğŸ’° Verkaufsmodell & RechteÃ¼bertragung

ğŸ“Œ **Preis:** Verhandelbar (VHB) â€“ Kontaktieren Sie uns fÃ¼r ein individuelles Angebot.

ğŸ“Œ **Einmaliger Kauf â€“ Volle RechteÃ¼bertragung:**
- **UneingeschrÃ¤nkte Nutzung** innerhalb des Unternehmens oder der Organisation
- **Anpassung & Modifikation** durch interne Entwickler
- **Kommerzielle Nutzung & Weiterverkauf**
- **Keine LizenzgebÃ¼hren** nach dem Kauf

ğŸ“Œ **Wichtiger Hinweis:** Dies ist ein einmaliger Verkauf des Quellcodes als selbst-gehostete Software-LÃ¶sung. **Kein Support oder Wartung nach dem Verkauf inklusive.** Eine umfassende Dokumentation zur internen Wartung, Anpassung und Weiterentwicklung wird bereitgestellt.

ğŸ“© **Interesse geweckt?**

Kontaktieren Sie uns direkt fÃ¼r weitere Informationen oder eine persÃ¶nliche Demo!




# ğŸš€ **WebCrawler-Pro**

## ğŸ“Œ **EinfÃ¼hrung**
WebCrawler-Pro ist ein leistungsstarkes Werkzeug zur **automatischen Extraktion, Verarbeitung und Bereitstellung von Web-Daten** Ã¼ber eine API. Es kombiniert **Web-Scraping, Datenverarbeitung, API-Integration, Sicherheit und Task-Planung** in einem einzigen System. Die geplante Task-AusfÃ¼hrung und das Monitoring erfolgen direkt Ã¼ber die Datenbank.  ZusÃ¤tzlich bietet WebCrawler-Pro eine **intuitive Web-OberflÃ¤che mit Streamlit**, um geplante Tasks komfortabel zu verwalten.

### ğŸ”¹ **Hauptfunktionen**
âœ… **Web-Scraping mit Selenium** (automatisiertes Abrufen von Webseiteninhalten)
âœ… **Gezielte Datenextraktion mit CSS-Selektoren** (inkl. Typkonvertierung & Datenbereinigung)
âœ… **Datenverarbeitung mit benutzerdefinierten Funktionen** (`processing.py`)
âœ… **RESTful API zur Bereitstellung der Daten** (mit detaillierten Fehlermeldungen)
âœ… **Automatisierte Task-Planung aus der Datenbank** (mit Monitoring & manueller AusfÃ¼hrung Ã¼ber die API)
âœ… **API-Authentifizierung per API-Key** ğŸ”‘
âœ… **Ratenbegrenzung zum Schutz vor Missbrauch** ğŸ›¡ï¸
âœ… **Caching zur Leistungssteigerung** âš¡
âœ… **Datenbankintegration mit SQLite** ğŸ—„ï¸ (mit Transaktionssicherheit)
âœ… **Erweiterbare SicherheitsmaÃŸnahmen gegen Path Traversal & CSS-Injection**
âœ… **Monitoring fÃ¼r geplante Tasks und API-Status Ã¼ber API-Endpunkte** ğŸ“Š (inkl. Start-/Endzeiten, Logs, Fehlerberichte, letzter/nÃ¤chster AusfÃ¼hrungszeit)
âœ… **Einfache Konfiguration Ã¼ber YAML-Dateien & Umgebungsvariablen** âš™ï¸
âœ… **Web-OberflÃ¤che mit Streamlit zur Taskverwaltung** ğŸ–¥ï¸

ğŸ“– Diese Dokumentation beschreibt die **Installation, Konfiguration und Nutzung** des Programms.

---

## ğŸ”§ **1. Installation**
### ğŸ“‚ **1.1 Voraussetzungen**
ğŸ“Œ **Erforderliche Software:**
- ğŸ **Python 3.7 oder hÃ¶her** *(empfohlen: 3.9+)*
- ğŸ“¦ **pip** *(Python-Paketmanager, sollte mit Python installiert sein)*
- ğŸŒ **Google Chrome + ChromeDriver** *(fÃ¼r Selenium-basiertes Scraping)*
- ğŸ§  **NLTK Data:** *(FÃ¼r Keyword-Extraktion: `python -m nltk.downloader stopwords`)*
- ğŸ–¥ï¸ **Streamlit:** *(FÃ¼r die Web-OberflÃ¤che: `pip install streamlit`)*

### ğŸ“¥ **1.2 AbhÃ¤ngigkeiten installieren**
FÃ¼hre folgenden Befehl aus, um alle benÃ¶tigten Pakete zu installieren:
```bash
pip install -r requirements.txt
```


---

## âš™ï¸ **2. Konfiguration**
Das Programm wird Ã¼ber **YAML-Dateien & Umgebungsvariablen** konfiguriert:
- ğŸ“„ **`config.yaml`** â†’ EnthÃ¤lt Einstellungen fÃ¼r Scraping, API, Datenbank & Sicherheit.
- ğŸ”‘ **`.env` Datei** â†’ Speichert API-Keys & andere sensible Informationen. *(Umgebungsvariablen haben Vorrang vor `config.yaml`.)*  API-Keys kÃ¶nnen auch direkt in `config.yaml` unter `api_keys` eingetragen werden.

### ğŸ›  **2.1 `config.yaml` (Beispiel)**
```yaml
scraping:
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
  timeout: 10

database:
  type: sqlite
  path: "data/webcrawler.db"

security:
  api_key_required: true
  rate_limit: 100
```

### ğŸ”‘ **2.2 `.env` Datei (Beispiel)**
```ini
API_KEY_1=mein_geheimer_api_key_1
API_KEY_2=mein_geheimer_api_key_2
API_KEY_3=mein_geheimer_api_key_3
```

---

## â–¶ï¸ **3. Nutzung**
### ğŸŒ **3.1 API-Modus** *(Startet den API-Server)*
```bash
python app.py --api
```

### ğŸ•µï¸ **3.2 Kommandozeilenmodus** *(Einzelnes Scraping ausfÃ¼hren)*
```bash
python app.py https://example.com [Optionen]
```

### â³ **3.3 Geplanter Modus** *(Automatische Tasks aus der Datenbank ausfÃ¼hren)*
```bash
python app.py
```
ğŸ“Œ **Hinweis:** Geplante Tasks werden aus der Datenbank (`webdata.db`, konfigurierbar in `config.yaml`) geladen und ausgefÃ¼hrt. 
---


## ğŸ›  **4. Benutzerdefinierte Datenverarbeitung (`processing.py`)**
ğŸ“Œ **ErmÃ¶glicht benutzerdefinierte Verarbeitung gescrapter Daten.**

**Beispiel:**
```python
def process_data(data: dict) -> dict:
    """
    Verarbeitet gescrapte Daten. Muss ein Dictionary zurÃ¼ckgeben.
    Falls `None` zurÃ¼ckgegeben wird, erscheint eine Warnung im Log.
    """
    if not isinstance(data, dict):
        return None

    processed_data = {k: v.strip() if isinstance(v, str) else v for k, v in data.items()}
    return processed_data
```

---

## ğŸ”’ **5. Sicherheitshinweise**
ğŸ“Œ **Schutzmechanismen:**
- ğŸ”‘ **API-Key-Authentifizierung** *(API-Endpunkte erfordern einen API-Key im Header)*
- ğŸ›¡ **Ratenbegrenzung** *(Maximal 100 Anfragen pro Minute, konfigurierbar in `config.yaml`)*
- ğŸš¨ **Path Traversal-Schutz** *(Verhindert unautorisierten Zugriff auf Dateien)*
- ğŸ” **CSS-Selektor-Validierung** *(Unsichere Selektoren werden ignoriert & protokolliert)*

**Beispiel fÃ¼r ein Sicherheitslog:**
```sh
2025-03-09 14:43:19,238 - WARNING - Unsicherer CSS-Selektor erkannt: div[onclick*=alert]
```

---

## ğŸ“Š **6. Fehlerbehandlung & Logging**
ğŸ“Œ **Wo werden Fehler protokolliert?**
- ğŸ–¥ **Konsolenausgabe** *(Standard, fÃ¼r schnelle Fehleranalyse)*
- ğŸ—‚ **Log-Datei `logs/webcrawler.log`** *(falls aktiviert)*

**Log-Level:**
- âœ… **INFO** â†’ Allgemeine Statusmeldungen
- âš ï¸ **WARNING** â†’ Sicherheitswarnungen
- âŒ **ERROR** â†’ Kritische Fehler


## ğŸ“¡ 7. API-Endpunkte
ğŸ“Œ **Ãœbersicht der wichtigsten API-Endpunkte (aktualisiert):**
- ğŸŒ `/api/v1/` â†’ API Root mit Ãœbersicht aller Endpunkte
- ğŸ“„ `/api/v1/fetch-html` â†’ HTML-Inhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ“„ `/api/v1/fetch-text` â†’ Textinhalt abrufen (Parameter: `url`, `stopwords`, `css-selectors`, `save_file`, `processing_function_path`)
- ğŸ”„ `/api/v1/scheduled-tasks` â†’ Aufgabenverwaltung (GET, POST, PUT, DELETE)
- ğŸ”„ `/api/v1/scheduled-tasks/<task_id>` â†’ Einzelne Task verwalten (GET, PUT, DELETE)
- ğŸ“Š `/api/v1/scheduled-tasks/status` â†’ Status aller geplanten Tasks
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/status` â†’ Status eines spezifischen Tasks  (inkl. letzter/nÃ¤chster AusfÃ¼hrungszeit und Fehlermeldungen)
- ğŸ“Š `/api/v1/scheduled-tasks/<task_id>/run` â†’ Manuelles AusfÃ¼hren eines Tasks
- âœ… `/api/v1/health` â†’ API Health Check


ğŸ” **Alle API-Endpunkte erfordern API-Authentifizierung!**

---
## **WeboberflÃ¤che**

Mit dem Befehl `streamlit run app.py -- --streamlit` starten Sie die WeboberflÃ¤che zur bequemen Verwaltung Ihrer Webcrawling-Tasks.  Sie benÃ¶tigen hierfÃ¼r einen gÃ¼ltigen API-Key.

**API-Key Eingabe:**

Nach dem Start der WeboberflÃ¤che werden Sie aufgefordert, Ihren API-Key einzugeben.  Dieser dient zur Authentifizierung und Autorisierung des Zugriffs auf die Funktionen der WeboberflÃ¤che. Geben Sie Ihren API-Key in das dafÃ¼r vorgesehene Feld ein und bestÃ¤tigen Sie mit Enter. Ein ungÃ¼ltiger API Key fÃ¼hrt zu einer Fehlermeldung.

**Anzeige geplanter Tasks:**

Die WeboberflÃ¤che listet alle geplanten Tasks Ã¼bersichtlich auf. FÃ¼r jeden Task werden folgende Informationen angezeigt:

* **Task ID:** Eindeutige Identifikationsnummer des Tasks.
* **URL:** Die zu crawlende Webseite.
* **Zeitplan:**  Definiert, wann der Task ausgefÃ¼hrt wird (z.B. stÃ¼ndlich, tÃ¤glich um 10:00, alle 30 Minuten).
* **Nur Text:**  Gibt an, ob nur der Textinhalt oder der gesamte HTML-Code extrahiert werden soll.
* **StopwÃ¶rter:**  Auflistung der StopwÃ¶rter, die bei der Keyword-Extraktion ignoriert werden.
* **CSS-Selektoren:**  Die verwendeten CSS-Selektoren zur gezielten Datenextraktion.
* **Datei speichern:**  Gibt an, ob die extrahierten Daten zusÃ¤tzlich zur Datenbank in einer Datei gespeichert werden sollen.
* **Verarbeitungsfunktion:**  Pfad zur benutzerdefinierten Verarbeitungsfunktion (optional).
* **Status:**  Aktueller Status des Tasks (z.B. `pending`, `running`, `success`, `failure`).
* **Letzte AusfÃ¼hrung:**  Zeitstempel der letzten AusfÃ¼hrung.
* **NÃ¤chste AusfÃ¼hrung:**  Zeitstempel der nÃ¤chsten geplanten AusfÃ¼hrung.
* **Fehlermeldung:**  Anzeige von etwaigen Fehlermeldungen bei der AusfÃ¼hrung des Tasks.


**Aktionen fÃ¼r jeden Task:**

* **LÃ¶schen:**  Ãœber den Button "Task [ID] lÃ¶schen" kann ein geplanter Task entfernt werden.
* **Sofort ausfÃ¼hren:** Mit dem Button "Task [ID] sofort ausfÃ¼hren" kann ein Task unabhÃ¤ngig vom Zeitplan manuell gestartet werden.  Der Status aktualisiert sich nach der AusfÃ¼hrung.

**HinzufÃ¼gen eines neuen Tasks:**

Im Bereich "Neuen Task hinzufÃ¼gen" kÃ¶nnen Sie neue Crawling-Tasks erstellen.  FÃ¼llen Sie die folgenden Felder aus:

* **URL:**  Die URL der zu crawlenden Webseite.
* **Zeitplan:**  Definieren Sie den AusfÃ¼hrungszeitplan des Tasks. GÃ¼ltige Formate sind: "stÃ¼ndlich", "tÃ¤glich um HH:MM" oder "alle X minuten".
* **Nur Text extrahieren:** Aktivieren Sie diese Option, um nur den Textinhalt der Webseite zu extrahieren.
* **StopwÃ¶rter:**  Geben Sie optional eine kommagetrennte Liste von StopwÃ¶rtern ein.
* **CSS-Selektoren:** FÃ¼gen Sie optional CSS-Selektoren im JSON-Format hinzu, um bestimmte Daten von der Webseite zu extrahieren.
* **Datei speichern:**  Aktivieren Sie diese Option, um die extrahierten Daten in einer Datei zu speichern.
* **Verarbeitungsfunktion:** Geben Sie optional den Pfad zu einer benutzerdefinierten Verarbeitungsfunktion an.


Nach dem AusfÃ¼llen der Felder klicken Sie auf "Task hinzufÃ¼gen", um den neuen Task zu speichern und zu planen.  Die WeboberflÃ¤che aktualisiert sich automatisch und zeigt den neu hinzugefÃ¼gten Task an.  Fehler bei der Eingabe werden direkt angezeigt.

```
```bash
streamlit run app.py -- --streamlit
```

![image](https://github.com/user-attachments/assets/d2b6b9aa-ebaa-4450-8870-0096207fb2e1)

---
---
## **Hier sind einige Beispieleingaben fÃ¼r die Streamlit-WeboberflÃ¤che deines WebCrawler-Pro**

**Beispiel 1: Einfacher Webseiten-Crawl**

* **URL:** `https://www.example.com`
* **Zeitplan:** `tÃ¤glich um 10:00`
* **Nur Text extrahieren:** (deaktiviert)
* **StopwÃ¶rter:**
* **CSS-Selektoren:**
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:**


Dieser Task crawlt tÃ¤glich um 10:00 Uhr die Webseite `https://www.example.com` und speichert den gesamten HTML-Inhalt in einer Datei.  Es werden keine StopwÃ¶rter, CSS-Selektoren oder Verarbeitungsfunktionen verwendet.

**Beispiel 2: Text-Extraktion mit StopwÃ¶rtern**

* **URL:** `https://www.wikipedia.org`
* **Zeitplan:** `alle 60 minuten`
* **Nur Text extrahieren:** (aktiviert)
* **StopwÃ¶rter:** `und, die, der, das, ist`
* **CSS-Selektoren:**
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:**


Dieser Task extrahiert stÃ¼ndlich den Textinhalt von `https://www.wikipedia.org`. Die angegebenen StopwÃ¶rter werden bei der Keyword-Extraktion ignoriert. Der extrahierte Text wird in einer Datei gespeichert.

**Beispiel 3: Datenextraktion mit CSS-Selektoren**

* **URL:** `https://www.amazon.de/`
* **Zeitplan:** `stÃ¼ndlich`
* **Nur Text extrahieren:** (deaktiviert)
* **StopwÃ¶rter:**
* **CSS-Selektoren:**
```json
{
  "product_title": "h2.a-size-mini a.a-link-normal span",
  "product_price": "span.a-price span.a-offscreen"
}
```
* **Datei speichern:** (deaktiviert)
* **Verarbeitungsfunktion:** `./PROCESSING_FUNCTIONS_DIR/#1 custom_processing.py`


Dieser Task crawlt stÃ¼ndlich Amazon und extrahiert Produkttitel und -preise mithilfe der angegebenen CSS-Selektoren. Die extrahierten Daten werden mit der angegebenen Verarbeitungsfunktion weiterverarbeitet und in der Datenbank gespeichert. Beachte, dass der Pfad zur Verarbeitungsfunktion relativ zum AusfÃ¼hrungsverzeichnis des Crawlers sein muss.


**Beispiel 4:  Kombination aller Funktionen**

* **URL:** `https://www.heise.de`
* **Zeitplan:** `tÃ¤glich um 06:00`
* **Nur Text extrahieren:** (aktiviert)
* **StopwÃ¶rter:** `mit, von, am, im`
* **CSS-Selektoren:** `{"article_title": "h2 a"}`
* **Datei speichern:** (aktiviert)
* **Verarbeitungsfunktion:** `./PROCESSING_FUNCTIONS_DIR/#2 custom_processing.py` (oder ein anderer gÃ¼ltiger Pfad)


Dieser Task kombiniert alle verfÃ¼gbaren Funktionen. Er crawlt tÃ¤glich um 6:00 Uhr heise.de, extrahiert den Textinhalt, filtert die angegebenen StopwÃ¶rter, extrahiert Artikeltitel mithilfe des CSS-Selektors, speichert den Textinhalt in einer Datei und verarbeitet die Daten mit der angegebenen Funktion.

**Wichtig:**

* **GÃ¼ltige Pfade fÃ¼r Verarbeitungsfunktionen:** Achte darauf, dass die Pfade zu den Verarbeitungsfunktionen korrekt und relativ zum AusfÃ¼hrungsverzeichnis des Crawlers angegeben sind.  Die Beispiele oben gehen davon aus, dass sich die `custom_processing.py` Dateien im Verzeichnis `PROCESSING_FUNCTIONS_DIR` befinden.
* **JSON-Format fÃ¼r CSS-Selektoren:** Die CSS-Selektoren mÃ¼ssen in einem gÃ¼ltigen JSON-Format angegeben werden.
* **Testen:** Teste die Eingaben grÃ¼ndlich, um sicherzustellen, dass sie die gewÃ¼nschten Ergebnisse liefern.
---
## ğŸ **9. WebCrawler-Pro**
âœ… **Automatisierte Task-Planung direkt aus der Datenbank**
âœ… **Erweiterbare Datenverarbeitung durch `processing.py`**
âœ… **Detaillierte Logging- & SicherheitsmaÃŸnahmen**
âœ… **Einfache Konfiguration Ã¼ber `config.yaml` & `.env`**

ğŸš€ **WebCrawler-Pro ist die ideale LÃ¶sung fÃ¼r produktives, sicheres und flexibles Web-Scraping!**





# WOFÃœR KANN MAN **WebCrawler-Pro** NUTZEN?

## **EinsatzmÃ¶glichkeiten fÃ¼r verschiedene Zielgruppen**

### **1ï¸âƒ£ Unternehmen & Startups**
**ğŸ“Š Markt- & Wettbewerbsanalyse**  
- Automatische Ãœberwachung der Konkurrenz-Webseiten (Preise, Produkte, Dienstleistungen).  
- Extraktion von **SEO-Daten** (Meta-Descriptions, Keywords, Ãœberschriften) zur Analyse von Mitbewerbern.
- Identifikation neuer Branchentrends durch **regelmÃ¤ÃŸiges Crawling von Nachrichtenportalen**.

**ğŸ“¢ Marketing & Lead-Generierung**  
- Sammlung von **potenziellen Kundenkontakten** aus Ã¶ffentlich zugÃ¤nglichen Quellen (z.B. Firmenverzeichnisse, Social Media, Events).
- Analyse von **Produktbewertungen und Rezensionen** auf Plattformen wie Amazon oder Trustpilot.
- Ãœberwachung von **Markennennungen** in Blogs oder Foren fÃ¼r gezieltes Online-Reputation-Management.

**âš–ï¸ Compliance & Risikomanagement**  
- Automatische ÃœberprÃ¼fung von **gesetzlichen Vorgaben auf regulatorischen Webseiten** (z.B. DSGVO-Updates, Ã„nderungen in AGBs von Partnern).
- **FrÃ¼hwarnsysteme** fÃ¼r negative Presseberichte oder Fake News Ã¼ber das eigene Unternehmen.
- Erkennung von **Plagiaten oder unerlaubter Nutzung eigener Inhalte** durch Web Scraping.

---

### **2ï¸âƒ£ Vereine & Organisationen**
**ğŸ“… Event- & Mitgliederverwaltung**  
- Automatische Aktualisierung von **Veranstaltungslisten** durch das Scrapen von Ticketseiten oder Event-Plattformen.
- **Monitoring von FÃ¶rderprogrammen** fÃ¼r gemeinnÃ¼tzige Organisationen und Vereine.
- **Automatisierte Newsletter-Erstellung** basierend auf gesammelten Blogposts und News aus der Branche.

**ğŸ“£ Ã–ffentlichkeitsarbeit & Fundraising**  
- Beobachtung von **sozialen Medien & Nachrichten**, um Trends fÃ¼r Fundraising-Kampagnen zu identifizieren.
- Sammlung von **Statistiken & Berichten**, die fÃ¼r FÃ¶rderantrÃ¤ge oder PrÃ¤sentationen genutzt werden kÃ¶nnen.
- Erkennung von **Fake News Ã¼ber die Organisation** und schnelle Reaktion durch Ãœberwachung relevanter Webseiten.

---

### **3ï¸âƒ£ Ã–ffentliche Einrichtungen & NGOs**
**ğŸ“œ Politische & gesellschaftliche Entwicklungen**  
- Kontinuierliche **Analyse von Gesetzestexten und neuen Vorschriften**, um schnell reagieren zu kÃ¶nnen.
- **Erkennung von Desinformationen & Fake News**, um gezielte AufklÃ¤rungskampagnen zu starten.
- **Wissenschaftliche Studien & Berichte** automatisch sammeln und katalogisieren.

**ğŸŒ Umwelt & Nachhaltigkeit**  
- Ãœberwachung von **Wetter- und Klimadaten** durch Scraping offizieller Wetterseiten.
- **Analyse von UmweltschutzmaÃŸnahmen** anderer LÃ¤nder zur Vergleichsanalyse.
- Sammlung von **offiziellen Berichten zu COâ‚‚-Emissionen** zur FÃ¶rderung von Nachhaltigkeitsprojekten.

---

### **4ï¸âƒ£ Privatpersonen & Blogger**
**ğŸ“œ Nachrichten & Information**  
- Automatische **Zusammenfassung relevanter Nachrichten** aus verschiedenen Quellen.
- **PersÃ¶nlicher News-Feed**, der individuell zusammengestellte Themenbereiche crawlt.
- **Verwaltung von Buch- oder Filmkritiken** durch automatisches Extrahieren von Rezensionen.

**ğŸ’° Finanzielle Vorteile**  
- **Automatische PreisÃ¼berwachung & SchnÃ¤ppchenalarm** fÃ¼r bestimmte Produkte.
- Vergleich von **Krypto- und Aktienkursen** mit historischen Daten.
- Automatische **Immobilienpreis-Analyse** durch Scraping von WohnungsbÃ¶rsen.

**ğŸ“– Wissen & Lernen**  
- Sammlung von **Wikipedia-Artikeln und wissenschaftlichen Studien** zu bestimmten Themen.
- Automatische Extraktion von **Online-Kursen & Lernmaterialien**.
- Erstellung eines **automatisierten Blog-Kalenders** mit geplanten BeitrÃ¤gen aus verschiedenen Quellen.

---

## **Ein vielseitiges Werkzeug fÃ¼r viele AnwendungsfÃ¤lle!**
**WebCrawler-Pro** ist eine leistungsstarke LÃ¶sung fÃ¼r Unternehmen, Vereine, Organisationen und Privatpersonen, die gezielt Informationen aus dem Internet extrahieren mÃ¶chten. Dank der umfangreichen KonfigurationsmÃ¶glichkeiten und der sicheren API-Schnittstelle lassen sich zahlreiche Aufgaben **automatisieren und optimieren**. ğŸš€



# **Softwarebewertung â€“ WebCrawler-Pro**  
**Autor: CipherCore**  
**Datum: MÃ¤rz 2025**  
**Version: 1.1**  

---

## **1. EinfÃ¼hrung**  
WebCrawler-Pro ist eine leistungsfÃ¤hige, vielseitige und sichere Anwendung zur **automatischen Extraktion, Verarbeitung und Bereitstellung von Web-Daten** Ã¼ber eine API. Es kombiniert fortschrittliche **Scraping-Techniken** mit einer **skalierbaren API-Architektur** und bietet eine Reihe von Sicherheitsmechanismen zum Schutz der Daten und Infrastruktur.  

Diese Bewertung analysiert die Software hinsichtlich **FunktionalitÃ¤t, Sicherheit, Performance, Skalierbarkeit, Wartbarkeit, Testabdeckung, Benutzerfreundlichkeit und Dokumentation**.

---

## **2. FunktionalitÃ¤t**  
### **2.1 Kernfunktionen**
âœ” **Web-Scraping:**  
- Nutzung von **Selenium** zum Abrufen von Webseiteninhalten.  
- Extraktion von **Text, Titeln, Metadaten, Ãœberschriften und SchlÃ¼sselwÃ¶rtern**.  
- UnterstÃ¼tzung fÃ¼r **CSS-Selektoren** zur gezielten Datenentnahme.  
- Automatisierte **Task-Planung** fÃ¼r regelmÃ¤ÃŸige Scraping-Prozesse.  

âœ” **API-Integration:**  
- Bereitstellung der gesammelten Daten Ã¼ber eine **RESTful API**.  
- **Authentifizierung per API-Key** zum Schutz der Endpunkte.  
- **Ratenbegrenzung**, um Missbrauch zu verhindern.  
- **Monitoring von geplanten Tasks** mit Statusberichten.  

âœ” **Datenverarbeitung & Speicherung:**  
- MÃ¶glichkeit zur **benutzerdefinierten Verarbeitung der gesammelten Daten**.  
- Speicherung in einer **SQLite-Datenbank** fÃ¼r persistente Datennutzung.  
- **Caching-Mechanismus**, um wiederholte Anfragen effizient zu handhaben.  

âœ” **SicherheitsmaÃŸnahmen:**  
- **Path Traversal-Schutz** zur Verhinderung von unautorisierten Dateioperationen.  
- **CSS-Injection-PrÃ¤vention** durch Validierung von Selektoren.  
- **Whitelist fÃ¼r Verarbeitungsfunktionen**, um unsicheren Code zu vermeiden.  

---

## **3. Sicherheit**  
**Bewertung: â˜…â˜…â˜…â˜…â˜… (5/5)**  

âœ” **API-Schutz:**  
- API-Authentifizierung Ã¼ber **API-Keys**.  
- **Ratenbegrenzung**, um DDoS- oder Brute-Force-Angriffe zu verhindern.  

âœ” **Datenvalidierung & Eingabekontrolle:**  
- **Pydantic** fÃ¼r strenge **Datenvalidierung** und Parsing.  
- **Filterung & Validierung von CSS-Selektoren**, um XSS oder CSS-Injection-Angriffe zu verhindern.  

âœ” **Datei- und Pfadsicherheit:**  
- **Path Traversal-PrÃ¤vention**, um sicherzustellen, dass keine unautorisierten Dateizugriffe stattfinden.  

âœ” **Sichere Speicherung:**  
- Verwendung einer **lokalen SQLite-Datenbank**, um Daten zentral zu verwalten.  
- MÃ¶glichkeit zur **Erweiterung auf sicherere Datenbanksysteme**.  

âœ… **Fazit:** Das System verfÃ¼gt Ã¼ber solide **SicherheitsmaÃŸnahmen**, die potenzielle Angriffsvektoren adressieren und die Software robust gegen Missbrauch machen.  

---

## **4. Performance und Skalierbarkeit**  
**Bewertung: â˜…â˜…â˜…â˜…â˜† (4/5)**  

âœ” **Effiziente Architektur:**  
- **Flask als API-Framework**, das leicht skalierbar ist.  
- **Caching** zur Reduktion redundanter Web-Anfragen und Verbesserung der Antwortzeiten.  
- Nutzung von **SQLite**, das fÃ¼r kleine bis mittlere Datenmengen gut geeignet ist.  

âœ” **OptimierungsmÃ¶glichkeiten:**  
- **Selenium kann ressourcenintensiv sein**, insbesondere bei hohem Anfragevolumen.  
- SQLite kÃ¶nnte bei **groÃŸer Datenlast** eine Limitierung darstellen (mÃ¶glicher Wechsel zu PostgreSQL oder MySQL).  
- **Task-Planung kÃ¶nnte von Threading oder einer Queue-Verarbeitung profitieren**, um Skalierbarkeit zu verbessern.  
- **Flask arbeitet synchron**, eine zukÃ¼nftige Erweiterung mit `asyncio` und `aiohttp` kÃ¶nnte die Performance weiter steigern.  

âœ… **Fazit:** Die aktuelle Performance ist fÃ¼r **mittelgroÃŸe Datenmengen optimiert**, aber fÃ¼r **groÃŸe und verteilte Systeme** kÃ¶nnte eine Anpassung der Architektur erforderlich sein.  

---

## **5. CodequalitÃ¤t & Wartbarkeit**  
**Bewertung: â˜…â˜…â˜…â˜…â˜… (5/5)**  

âœ” **Strukturierter & sauberer Code:**  
- **Modularisierung:** Klare Trennung zwischen Scraping, API und Datenverarbeitung.  
- **Einsatz von Konfigurationsdateien (.env, YAML)**, um Anpassungen ohne CodeÃ¤nderungen zu ermÃ¶glichen.  
- **Pydantic fÃ¼r Datenvalidierung**, was Fehlerquellen reduziert.  

âœ” **Hohe Wartungsfreundlichkeit:**  
- **Logging & Monitoring-Funktionen** erleichtern Debugging & Fehlerbehebung.  
- **Klare Tests fÃ¼r kritische Funktionen**, die eine zuverlÃ¤ssige Wartung ermÃ¶glichen.  
- **Detaillierte Dokumentation** macht zukÃ¼nftige Erweiterungen einfach.  

âœ… **Fazit:** Der Code ist **gut strukturiert, flexibel erweiterbar und leicht wartbar** â€“ ein groÃŸer Vorteil fÃ¼r langfristige Nutzung und Skalierung.  

---

## **6. Benutzerfreundlichkeit**  
**Bewertung: â˜…â˜…â˜…â˜…â˜… (5/5)**  

âœ” **Intuitive Bedienung:**  
- **Streamlit-Web-OberflÃ¤che**, die eine einfache Verwaltung von Scraping-Tasks ermÃ¶glicht.  
- **API-Dokumentation mit Beispielen**, um schnelle Integration zu gewÃ¤hrleisten.  

âœ” **Automatisierung & Konfiguration:**  
- **Task-Planung direkt Ã¼ber UI** mÃ¶glich.  
- **Konfigurierbare YAML- und `.env`-Dateien**, um die Software an individuelle BedÃ¼rfnisse anzupassen.  

âœ” **Fehlermeldungen & Logging:**  
- **Detaillierte Logging-Funktionen** helfen bei der Fehlerbehebung.  
- **Fehlermeldungen sind klar und verstÃ¤ndlich**, um Probleme schnell zu lÃ¶sen.  

âœ… **Fazit:** Die Software bietet eine **intuitive NutzerfÃ¼hrung** mit einer **benutzerfreundlichen Web-OberflÃ¤che und umfassender Konfigurierbarkeit**.  

---

## **7. Testabdeckung & QualitÃ¤tssicherung**  
**Bewertung: â˜…â˜…â˜…â˜…â˜… (5/5)**  

âœ” **Umfassende Testsuite:**  
- **Unit-Tests fÃ¼r zentrale Funktionen** (z. B. URL-Validierung, HTML-Parsing, API-Endpunkte).  
- **Sicherheitstests fÃ¼r Path Traversal & CSS-Selektor-Validierung**.  
- **Mocking fÃ¼r externe AbhÃ¤ngigkeiten (Webseiteninhalte, Dateisystem, API-Aufrufe)**.  

âœ” **Automatisierte TestausfÃ¼hrung:**  
- Nutzung von **unittest** fÃ¼r konsistente Tests.  
- Tests kÃ¶nnen mit folgendem Befehl ausgefÃ¼hrt werden:  
```sh
python -m unittest discover tests
```

âœ… **Fazit:** Sehr gute Testabdeckung mit Fokus auf **Sicherheits-, Integrations- und Funktionstests**.  

---

## **8. Fazit & Gesamtbewertung**  
### **Gesamtbewertung: 4,9 / 5 Sterne â­â­â­â­â­**  

âœ… **Empfehlung:** Diese Software ist **hochwertig, sicher und flexibel** und eignet sich ideal fÃ¼r **mittelgroÃŸe bis groÃŸe Web-Scraping-Projekte mit API-Integration**. ğŸš€



